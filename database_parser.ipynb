{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97ef963-3236-4faf-a941-d16a2a913ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import stat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "import urllib.request as ur\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8660f-f070-4218-b96e-9dbfc65b85fa",
   "metadata": {},
   "source": [
    "# API calls\n",
    "\n",
    "1. [Rainfall](https://api.data.gov.sg/v1/environment/rainfall)\n",
    "2. [Air temperature](https://api.data.gov.sg/v1/environment/air-temperature)\n",
    "3. [Wind Speed](https://api.data.gov.sg/v1/environment/wind-speed)\n",
    "4. [Air pollution](https://api.data.gov.sg/v1/environment/psi)\n",
    "\n",
    "The API is used by adding the suffix, `?date={to be added}` after the links above. This then looks up the data from the API which is returned as a json file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e91ea6b",
   "metadata": {},
   "source": [
    "The `parse_api` function is called to do two things. These are \n",
    "\n",
    "1. read the data from the url using the api\n",
    "2. converting that data from the json into a python object\n",
    "\n",
    "Objective 1 is done through the `urllib.request` package. First, the correct url is given to a `urllib.request` object. Once the correct url is read, the obj is then read and parsed with a json reader from the python package `json`. The read json, now called `data_obj` is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c78b3075-2a53-4222-bea2-bf76709c370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_api(url, year):\n",
    "    url =  url.format(str(year))\n",
    "    # print(url)\n",
    "    fileobj = ur.urlopen(url)\n",
    "    readobj = fileobj.read()\n",
    "    data_obj = json.loads(readobj)\n",
    "    return data_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1873c5",
   "metadata": {},
   "source": [
    "The `get_dates` function is called to obtain all the dates between the start and end date using the `datetime` package. The function takes the start and end date as inputs, and returns all dates inclusive of the end date in one day intervals. \n",
    "\n",
    "*TO-DO*\n",
    "- [x] Add in a way to set user defined date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78356206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(sdate, edate, interval = 1):\n",
    "    delta = edate - sdate       # as timedelta\n",
    "    dates = []\n",
    "\n",
    "    for i in range(0, delta.days + 1, interval):\n",
    "        day = sdate + timedelta(days=i)\n",
    "        dates.append(day)\n",
    "    \n",
    "    return np.array(dates, dtype = object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620ba26",
   "metadata": {},
   "source": [
    "The `get_stations` function takes the read `data_obj` and obtains all metadata from `data_obj` related to the identity and location of the reporting stations. This is then converted into a dataframe, and returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd95abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stations(data_obj):\n",
    "    stations = data_obj[\"metadata\"][\"stations\"]\n",
    "    df = pd.DataFrame.from_dict(stations)\n",
    "\n",
    "    locations = df['location'].to_numpy()\n",
    "    df['longitude'] = np.zeros(locations.size)\n",
    "    df['latitude'] = np.zeros(locations.size)\n",
    "\n",
    "    for i in range(locations.size):\n",
    "        df.loc[i, 'longitude'] = float(locations[i]['longitude'])\n",
    "        df.loc[i, 'latitude'] = float(locations[i]['latitude'])\n",
    "\n",
    "    df = df.drop(['location'], axis = 1)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d9c51",
   "metadata": {},
   "source": [
    "The `get_data_day` function takes in the information of the stations and the `data_obj` and returns a dataframe that represents all rainfall data obtained for that day. Usually data is taken at 5 minute intervals throughout the day from all stations. \n",
    "\n",
    "The flow of this function is to \n",
    "\n",
    "1. Obtain all the station id's\n",
    "2. Obtain the dictionary of all rainfall data that day from the json file\n",
    "3. Extract the time that the reading was taken\n",
    "4. Extract all rainfall data from the stations throughout the dat \n",
    "5. Write them into a dataframe\n",
    "\n",
    "Return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee0ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_day(data_obj, stations):\n",
    "    keys = np.array(stations['id'])\n",
    "    precipitation_raw = data_obj[\"items\"]\n",
    "    n = len(precipitation_raw)\n",
    "    t = np.zeros(n, dtype = object)\n",
    "    df = pd.DataFrame(np.zeros((n, keys.size)), columns= keys)\n",
    "\n",
    "    for i, d_obj in enumerate(precipitation_raw):\n",
    "        t[i] = d_obj['timestamp'].split('+')[0].split('T')[-1]\n",
    "        list_timestep = d_obj['readings']\n",
    "        for j in range(len(list_timestep)):\n",
    "            dict_key = list_timestep[j]['station_id']\n",
    "            value = list_timestep[j]['value']\n",
    "            df[dict_key].iloc[i] += value\n",
    "    \n",
    "    df['Time'] = t\n",
    "    df = df.set_index('Time')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077f284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs():\n",
    "    sdate = sys.argv[1]\n",
    "    edate = sys.argv[2]\n",
    "    if len(sys.argv) < 4:\n",
    "        save_folder = 'raw_data'\n",
    "    else:\n",
    "        save_folder = sys.argv[3]\n",
    "    return sdate, edate, save_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92669d01",
   "metadata": {},
   "source": [
    "*TO-DO*\n",
    "1. Add command line input system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c387a8a5-f720-4bcb-b1be-13becd92bd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists\n",
      "2018-03-01\n",
      "2018-03-06\n",
      "2018-03-11\n",
      "2018-03-16\n",
      "2018-03-21\n",
      "2018-03-26\n",
      "2018-03-31\n",
      "2018-04-05\n",
      "2018-04-10\n",
      "2018-04-15\n",
      "No valid stations on data from 2018-04-15\n",
      "2018-04-20\n",
      "2018-04-25\n",
      "2018-04-30\n",
      "2018-05-05\n",
      "2018-05-10\n",
      "2018-05-15\n",
      "2018-05-20\n",
      "2018-05-25\n",
      "2018-05-30\n",
      "2018-06-04\n",
      "2018-06-09\n",
      "2018-06-14\n",
      "2018-06-19\n",
      "2018-06-24\n",
      "2018-06-29\n",
      "2018-07-04\n",
      "2018-07-09\n",
      "2018-07-14\n",
      "2018-07-19\n",
      "2018-07-24\n",
      "2018-07-29\n",
      "2018-08-03\n",
      "2018-08-08\n",
      "2018-08-13\n",
      "2018-08-18\n",
      "2018-08-23\n",
      "2018-08-28\n",
      "2019-03-01\n",
      "2019-03-06\n",
      "2019-03-11\n",
      "2019-03-16\n",
      "2019-03-21\n",
      "2019-03-26\n",
      "2019-03-31\n",
      "2019-04-05\n",
      "2019-04-10\n",
      "2019-04-15\n",
      "2019-04-20\n",
      "2019-04-25\n",
      "2019-04-30\n",
      "2019-05-05\n",
      "2019-05-10\n",
      "2019-05-15\n",
      "2019-05-20\n",
      "2019-05-25\n",
      "2019-05-30\n",
      "2019-06-04\n",
      "2019-06-09\n",
      "2019-06-14\n",
      "2019-06-19\n",
      "2019-06-24\n",
      "2019-06-29\n",
      "2019-07-04\n",
      "2019-07-09\n",
      "2019-07-14\n",
      "2019-07-19\n",
      "2019-07-24\n",
      "No valid stations on data from 2019-07-24\n",
      "2019-07-29\n",
      "2019-08-03\n",
      "2019-08-08\n",
      "2019-08-13\n",
      "2019-08-18\n",
      "2019-08-23\n",
      "2019-08-28\n",
      "2020-03-01\n",
      "2020-03-06\n",
      "2020-03-11\n",
      "2020-03-16\n",
      "2020-03-21\n",
      "2020-03-26\n",
      "2020-03-31\n",
      "2020-04-05\n",
      "2020-04-10\n",
      "2020-04-15\n",
      "2020-04-20\n",
      "2020-04-25\n",
      "2020-04-30\n",
      "2020-05-05\n",
      "2020-05-10\n",
      "2020-05-15\n",
      "2020-05-20\n",
      "2020-05-25\n",
      "2020-05-30\n",
      "2020-06-04\n",
      "2020-06-09\n",
      "No valid stations on data from 2020-06-09\n",
      "2020-06-14\n",
      "2020-06-19\n",
      "2020-06-24\n",
      "2020-06-29\n",
      "2020-07-04\n",
      "2020-07-09\n",
      "2020-07-14\n",
      "2020-07-19\n",
      "2020-07-24\n",
      "2020-07-29\n",
      "2020-08-03\n",
      "2020-08-08\n",
      "2020-08-13\n",
      "2020-08-18\n",
      "2020-08-23\n",
      "2020-08-28\n",
      "2021-03-01\n",
      "2021-03-06\n",
      "2021-03-11\n",
      "2021-03-16\n",
      "2021-03-21\n",
      "2021-03-26\n",
      "2021-03-31\n",
      "2021-04-05\n",
      "2021-04-10\n",
      "2021-04-15\n",
      "2021-04-20\n",
      "2021-04-25\n",
      "2021-04-30\n",
      "2021-05-05\n",
      "2021-05-10\n",
      "2021-05-15\n",
      "2021-05-20\n",
      "2021-05-25\n",
      "2021-05-30\n",
      "2021-06-04\n",
      "2021-06-09\n",
      "2021-06-14\n",
      "2021-06-19\n",
      "2021-06-24\n",
      "2021-06-29\n",
      "2021-07-04\n",
      "2021-07-09\n",
      "2021-07-14\n",
      "2021-07-19\n",
      "2021-07-24\n",
      "2021-07-29\n",
      "2021-08-03\n",
      "2021-08-08\n",
      "2021-08-13\n",
      "2021-08-18\n",
      "2021-08-23\n",
      "2021-08-28\n",
      "2022-03-01\n",
      "2022-03-06\n",
      "2022-03-11\n",
      "2022-03-16\n",
      "2022-03-21\n",
      "2022-03-26\n",
      "2022-03-31\n",
      "2022-04-05\n",
      "2022-04-10\n",
      "2022-04-15\n",
      "2022-04-20\n",
      "2022-04-25\n",
      "2022-04-30\n",
      "2022-05-05\n",
      "2022-05-10\n",
      "2022-05-15\n",
      "2022-05-20\n",
      "2022-05-25\n",
      "2022-05-30\n",
      "2022-06-04\n",
      "2022-06-09\n",
      "2022-06-14\n",
      "2022-06-19\n",
      "2022-06-24\n",
      "2022-06-29\n",
      "2022-07-04\n",
      "2022-07-09\n",
      "2022-07-14\n",
      "2022-07-19\n",
      "2022-07-24\n",
      "2022-07-29\n",
      "2022-08-03\n",
      "2022-08-08\n",
      "2022-08-13\n",
      "2022-08-18\n",
      "2022-08-23\n",
      "No valid stations on data from 2022-08-23\n",
      "2022-08-28\n",
      "No valid stations on data from 2022-08-28\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.data.gov.sg/v1/environment/rainfall?date={0}\"\n",
    "save_folder = 'raw_data'\n",
    "\n",
    "# sdate, edate, save_folder = inputs()\n",
    "\n",
    "try:\n",
    "    os.mkdir('./{0}'.format(save_folder))\n",
    "except FileExistsError:\n",
    "    print('Folder exists')\n",
    "\n",
    "# sdate = date(2022, 1, 1)   # start date\n",
    "# edate = date.today()   # end date\n",
    "# dates = get_dates(sdate, edate)\n",
    "\n",
    "years = np.arange(2018, 2023)\n",
    "# months = np.arange(6, 9)\n",
    "\n",
    "dates = []\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    sdate = date(year, 3, 1)\n",
    "    edate = date(year, 9, 1)\n",
    "\n",
    "    temp_date = get_dates(sdate, edate, interval = 5)\n",
    "\n",
    "    for d in temp_date:\n",
    "        dates.append(d)\n",
    "\n",
    "# sdate = date(int(*sdate.split(\"_\")))\n",
    "# edate = date(int(*edate.split(\"_\")))\n",
    "\n",
    "for i, d in enumerate(dates):\n",
    "    data_obj = parse_api(url, d)\n",
    "    print(d)\n",
    "    if len(data_obj['metadata']['stations']) == 0:\n",
    "        print('No valid stations on data from', str(d))\n",
    "        continue\n",
    "    else:\n",
    "        stations = get_stations(data_obj)\n",
    "        precipitation = get_data_day(data_obj, stations)\n",
    "        stations.to_csv('./{0}/{1}_stations.csv'.format(save_folder, d))\n",
    "        precipitation.to_csv('./{0}/{1}_precipitation.csv'.format(save_folder, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167980ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(2000, 2023)\n",
    "months = np.arange(6, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b23504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c79cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
